{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "intellectual-hobby",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helper as h\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import re\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as ctb\n",
    "\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "from functools import partial\n",
    "from hyperopt import hp\n",
    "from hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "joined-thanks",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feats(df):\n",
    "    feats = df.select_dtypes(include=[int, float]).columns \n",
    "    return feats[ (feats != 'CO') & (feats != 'id')  & (feats != 'CO_log') ].values\n",
    "\n",
    "def get_X(df):\n",
    "    return df[ get_feats(df) ].values\n",
    "\n",
    "def get_y(df, target_var='CO'):\n",
    "    return df[target_var].values\n",
    "\n",
    "def get_models():\n",
    "    return [\n",
    "        ('dummy_mean', DummyRegressor(strategy='mean')),\n",
    "        ('dummy_median', DummyRegressor(strategy='median'))\n",
    "    ]\n",
    "\n",
    "def run_cv(model, X, y, folds=4, target_log=False,cv_type=KFold, success_metric=mean_absolute_error):\n",
    "    cv = cv_type(n_splits=folds)\n",
    "    \n",
    "    scores = []\n",
    "    for train_idx, test_idx in cv.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        if target_log:\n",
    "            y_train = np.log(y_train)\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        if target_log:\n",
    "            y_pred = np.exp(y_pred)\n",
    "            y_pred[y_pred < 0] = 0 #czasem może być wartość ujemna\n",
    "\n",
    "        score = success_metric(y_test, y_pred)\n",
    "        scores.append( score )\n",
    "        \n",
    "    return np.mean(scores), np.std(scores)\n",
    "\n",
    "\n",
    "def plot_learning_curve(model, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5), target_log=False):\n",
    "    \n",
    "    plt.figure(figsize=(12,8))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:plt.ylim(*ylim)\n",
    "\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    \n",
    "    if target_log:\n",
    "        y = np.log(y)\n",
    "    \n",
    "    def my_scorer(model, X, y):\n",
    "        y_pred = model.predict(X)\n",
    "        \n",
    "        if target_log:\n",
    "            y = np.exp(y)\n",
    "            y_pred = np.exp(y_pred)\n",
    "            y_pred[ y_pred<0 ] = 0\n",
    "        \n",
    "        return mean_absolute_error(y, y_pred)\n",
    "\n",
    "        \n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        model, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring=my_scorer)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    plt.grid()\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "\n",
    "def run(train, plot_lc=False, folds=3, ylim=(0, 2), target_log=False):\n",
    "    X, y  = get_X(train), get_y(train)\n",
    "\n",
    "    for model_name, model in get_models():\n",
    "        score_mean, score_std = run_cv(model, X, y, folds=folds, target_log=target_log)\n",
    "        print(\"[{0}]: {1} +/-{2}\".format(model_name, score_mean, score_std))\n",
    "        sys.stdout.flush() #wypisujemy wynik natychmiast, bez buforowania\n",
    "#         eli5.show_weights(model, feature_names=get_feats(train))\n",
    "#         if False == plot_lc: continue\n",
    "#         plt = plot_learning_curve(model, model_name, X, y, ylim=ylim, cv=folds, target_log=target_log)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "charming-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "## sqrt i log1p pogorszyły wynik z 0.41010 do  0.42367\n",
    "## sprawdzić samo sqrt i samo log1p czy pomogą czyli 2 submisiony jeszcze jutro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "outstanding-acceptance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):\n",
    "    df['AT_AH'] = df.AT/df.AH\n",
    "    df['AT_AP'] = df.AT/df.AP\n",
    "    df['AFDP_GTEP'] = df.AFDP/df.GTEP\n",
    "    df['TIT_TAT'] = df.TIT/df.TAT\n",
    "    df['GTEP_TEY'] = df.GTEP/df.TEY\n",
    "    df['NOX_AP'] = df.NOX/df.AP\n",
    "    df['TEY_NOX'] = df.TEY/df.NOX   \n",
    "    df['AH_NOX_SUM'] = df.AH+df.NOX\n",
    "    \n",
    "#     for feat in tqdm(get_feats(df)):\n",
    "#         if df[feat].skew() < 0.25: continue\n",
    "# #         df[feat + '_sqrt'] = np.sqrt( minmax_scale(df[feat]) )\n",
    "#         df[feat + '_log1p'] = np.log1p( minmax_scale(df[feat]) )\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "matched-female",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adequate-substance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[et-n200]: 0.4469605258527911 +/-0.023463477390586008\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_hdf(\"../input/train_power_plant.h5\")\n",
    "test = pd.read_hdf(\"../input/test_power_plant.h5\")\n",
    "train['CO_log'] = np.log( train['CO'] + 2 )\n",
    "train = feature_engineering(train)\n",
    "X_train,y_train = get_X(train), get_y(train)\n",
    "def get_models():\n",
    "    return [\n",
    "#         ('dummy_mean', DummyRegressor(strategy='mean')),\n",
    "#         ('dummy_median', DummyRegressor(strategy='median')),\n",
    "#         ('dt-5md', DecisionTreeRegressor(max_depth=5)),\n",
    "        ('et-n200',  ExtraTreesRegressor(n_estimators=200,random_state=0)),\n",
    "    ]\n",
    "\n",
    "run(train,folds=3, plot_lc=True, target_log = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "processed-maria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'AT', 'AP', 'AH', 'AFDP', 'GTEP', 'TIT', 'TAT', 'TEY', 'CDP',\n",
       "       'NOX', 'CO', 'CO_log', 'AT_AH', 'AT_AP', 'AFDP_GTEP', 'TIT_TAT',\n",
       "       'GTEP_TEY', 'NOX_AP', 'TEY_NOX', 'AH_NOX_SUM'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "respiratory-departure",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = feature_engineering(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proper-viewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = get_feats(train)\n",
    "X_train = train[feats].values\n",
    "y_train = train[\"CO_log\"].values\n",
    "\n",
    "X_test = test[feats].values\n",
    "\n",
    "model = ExtraTreesRegressor(n_estimators=200,max_depth=45, min_samples_leaf = 1,random_state=0)\n",
    "model.fit(X_train, y_train)\n",
    "test[\"CO_log\"] = model.predict(X_test)\n",
    "test['CO'] = np.exp(test['CO_log']) - 2\n",
    "test[ [\"id\", \"CO\"] ].to_csv(\"../output/etregressor_log_fe_20220401_08.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-judges",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
